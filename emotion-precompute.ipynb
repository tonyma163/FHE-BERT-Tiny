{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Intent Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precompute the mean, var, vy, normbias for each LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonyma/code/FHE-BERT-Tiny/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/tonyma/code/FHE-BERT-Tiny/env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gokuls/BERT-tiny-emotion-intent\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gokuls/BERT-tiny-emotion-intent\")\n",
    "\n",
    "model.eval()\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"dair-ai/emotion\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16000/16000 [00:00<00:00, 48583.18 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], truncation=False, padding=False)\n",
    "tokenized_valid_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Check dataset max tokenized length\n",
    "max_length = max(len(sample) for sample in tokenized_valid_dataset['input_ids'])\n",
    "print(f\"Max length: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16000/16000 [02:29<00:00, 107.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "mean_distribution_0_0 = []\n",
    "var_distribution_0_0 = []\n",
    "vy_distribution_0_0 = []\n",
    "normbias_distribution_0_0 = []\n",
    "\n",
    "mean_distribution_0_1 = []\n",
    "var_distribution_0_1 = []\n",
    "vy_distribution_0_1 = []\n",
    "normbias_distribution_0_1 = []\n",
    "\n",
    "mean_distribution_1_0 = []\n",
    "var_distribution_1_0 = []\n",
    "vy_distribution_1_0 = []\n",
    "normbias_distribution_1_0 = []\n",
    "\n",
    "mean_distribution_1_1 = []\n",
    "var_distribution_1_1 = []\n",
    "vy_distribution_1_1 = []\n",
    "normbias_distribution_1_1 = []\n",
    "\n",
    "for sentence in tqdm(train_dataset['text']):\n",
    "    text = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "\n",
    "    tokenized = tokenizer(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "    # Embeddings\n",
    "    x = model.bert.embeddings(tokens_tensor, segments_tensors)\n",
    "    original_input_tensor = x.double()\n",
    "    input_tensor = x.double()\n",
    "\n",
    "    # Self-Attention\n",
    "    fin = model.bert.encoder.layer[0].attention.self(x)[0].double()\n",
    "\n",
    "    w_output_dense = model.bert.encoder.layer[0].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[0].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    mean_0_0 = []\n",
    "    var_0_0 = []\n",
    "\n",
    "    fin3_whole = []\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        current_mean = torch.mean(fin2.squeeze()).item()\n",
    "        current_var = 1 / math.sqrt(torch.var(fin2.squeeze()).item())\n",
    "\n",
    "        # save mean and variance\n",
    "        mean_0_0.append(current_mean)\n",
    "        var_0_0.append(current_var)\n",
    "\n",
    "        fin3_corr = (fin2.squeeze() - current_mean) * current_var\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        # calculate vy\n",
    "        vy = w_output_layernorm * current_var\n",
    "        vy = vy.squeeze(0)\n",
    "        expanded_vy = vy.unsqueeze(1).repeat(1, max_length) # [128, 55]\n",
    "        padding_length = 128 - max_length\n",
    "        padding = torch.zeros(128, padding_length, dtype=vy.dtype)\n",
    "        vy_expanded = torch.cat((expanded_vy, padding), dim=1)  # Shape: [128, 128]\n",
    "        \n",
    "        # save vy & normbias\n",
    "        normbias = b_output_layernorm\n",
    "        vy_distribution_0_0.append(vy_expanded)\n",
    "        normbias_distribution_0_0.append(normbias)\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr)\n",
    "\n",
    "    mean_distribution_0_0.append(np.array(mean_0_0))\n",
    "    var_distribution_0_0.append(np.array(var_0_0))\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[0].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].intermediate.dense.bias\n",
    "\n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[0].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "\n",
    "    mean_0_1 = []\n",
    "    var_0_1 = []\n",
    "\n",
    "    fin7_whole = []\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin7 = fin_6.squeeze()[i]\n",
    "\n",
    "        current_mean = torch.mean(fin7.squeeze()).item()\n",
    "        current_var = 1 / math.sqrt(torch.var(fin7.squeeze()).item())\n",
    "\n",
    "        # save mean and variance\n",
    "        mean_0_1.append(current_mean)\n",
    "        var_0_1.append(current_var)\n",
    "\n",
    "        fin7_corr = (fin7.squeeze() - current_mean) * current_var\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        # calculate vy\n",
    "        vy = w_output_layernorm * current_var\n",
    "        vy = vy.squeeze(0)\n",
    "        expanded_vy = vy.unsqueeze(1).repeat(1, max_length) # [128, 55]\n",
    "        padding_length = 128 - max_length\n",
    "        padding = torch.zeros(128, padding_length, dtype=vy.dtype)\n",
    "        vy_expanded = torch.cat((expanded_vy, padding), dim=1)  # Shape: [128, 128]\n",
    "        \n",
    "        # save vy & normbias\n",
    "        normbias = b_output_layernorm\n",
    "        vy_distribution_0_1.append(vy_expanded)\n",
    "        normbias_distribution_0_1.append(normbias)\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin7_whole.append(fin7_corr)\n",
    "        \n",
    "\n",
    "    mean_distribution_0_1.append(np.array(mean_0_1))\n",
    "    var_distribution_0_1.append(np.array(var_0_1))\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    original_input_tensor = fin7_whole\n",
    "\n",
    "    fin = model.bert.encoder.layer[1].attention.self(fin7_whole)[0].double()\n",
    "\n",
    "    w_output_dense = model.bert.encoder.layer[1].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[1].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    mean_1_0 = []\n",
    "    var_1_0 = []\n",
    "\n",
    "    fin3_whole = []\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        current_mean = torch.mean(fin2.squeeze()).item()\n",
    "        current_var = 1 / math.sqrt(torch.var(fin2.squeeze()).item())\n",
    "\n",
    "        mean_1_0.append(current_mean)\n",
    "        var_1_0.append(current_var)\n",
    "\n",
    "        fin3_corr = (fin2.squeeze() - current_mean) * current_var\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        # calculate vy\n",
    "        vy = w_output_layernorm * current_var\n",
    "        vy = vy.squeeze(0)\n",
    "        expanded_vy = vy.unsqueeze(1).repeat(1, max_length) # [128, 55]\n",
    "        padding_length = 128 - max_length\n",
    "        padding = torch.zeros(128, padding_length, dtype=vy.dtype)\n",
    "        vy_expanded = torch.cat((expanded_vy, padding), dim=1)  # Shape: [128, 128]\n",
    "        \n",
    "        # save vy & normbias\n",
    "        normbias = b_output_layernorm\n",
    "        vy_distribution_1_0.append(vy_expanded)\n",
    "        normbias_distribution_1_0.append(normbias)\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr)\n",
    "\n",
    "    mean_distribution_1_0.append(np.array(mean_1_0))\n",
    "    var_distribution_1_0.append(np.array(var_1_0))\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[1].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].intermediate.dense.bias\n",
    "\n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[1].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    mean_1_1 = []\n",
    "    var_1_1 = []\n",
    "\n",
    "    fin7_whole = []\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin7 = fin_6.squeeze()[i]\n",
    "\n",
    "        current_mean = torch.mean(fin7.squeeze()).item()\n",
    "        current_var = 1 / math.sqrt(torch.var(fin7.squeeze()).item())\n",
    "\n",
    "        mean_1_1.append(current_mean)\n",
    "        var_1_1.append(current_var)\n",
    "\n",
    "        fin7_corr = (fin7.squeeze() - current_mean) * current_var\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        \n",
    "        # calculate vy\n",
    "        vy = w_output_layernorm * current_var\n",
    "        vy = vy.squeeze(0)\n",
    "        expanded_vy = vy.unsqueeze(1).repeat(1, max_length) # [128, 55]\n",
    "        padding_length = 128 - max_length\n",
    "        padding = torch.zeros(128, padding_length, dtype=vy.dtype)\n",
    "        vy_expanded = torch.cat((expanded_vy, padding), dim=1)  # Shape: [128, 128]\n",
    "        \n",
    "        # save vy & normbias\n",
    "        normbias = b_output_layernorm\n",
    "        vy_distribution_1_1.append(vy_expanded)\n",
    "        normbias_distribution_1_1.append(normbias)\n",
    "        \n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin7_whole.append(fin7_corr.unsqueeze(0))\n",
    "\n",
    "    mean_distribution_1_1.append(np.array(mean_1_1))\n",
    "    var_distribution_1_1.append(np.array(var_1_1))\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing layer 0\n",
      "Processing layer 1\n",
      "Processing layer 2\n",
      "Processing layer 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "precomputed_mean_0_0 = []\n",
    "precomputed_var_0_0 = []\n",
    "precomputed_mean_0_1 = []\n",
    "precomputed_var_0_1 = []\n",
    "precomputed_mean_1_0 = []\n",
    "precomputed_var_1_0 = []\n",
    "precomputed_mean_1_1 = []\n",
    "precomputed_var_1_1 = []\n",
    "\n",
    "precomputed_normbias_0_0 = []\n",
    "precomputed_normbias_0_1 = []\n",
    "precomputed_normbias_1_0 = []\n",
    "precomputed_normbias_1_1 = []\n",
    "\n",
    "current_mean_distribution = mean_distribution_0_0\n",
    "current_var_distribution = var_distribution_0_0\n",
    "current_normbias_distribution = normbias_distribution_0_0\n",
    "\n",
    "for layer in range(4):\n",
    "    print(f\"Processing layer {layer}\")\n",
    "    # Calculate mean for each token position across all samples\n",
    "    max_length = max(len(sample) for sample in current_mean_distribution)\n",
    "    total_means = np.zeros(128)  # Initialize with 128 positions\n",
    "    total_vars = np.zeros(128)  # Initialize with 128 positions\n",
    "    total_normbias = np.zeros(128)  # Initialize with 128 positions\n",
    "    counts = np.zeros(128)  # Initialize with 128 positions\n",
    "\n",
    "    for sample in current_mean_distribution:\n",
    "        for i, value in enumerate(sample):\n",
    "            if i < 128:  # Only consider up to 128 positions\n",
    "                total_means[i] += value\n",
    "                #total_vars[i] += np.var(value)\n",
    "                #total_normbias[i] += np.mean(value)\n",
    "                counts[i] += 1\n",
    "    \n",
    "    for sample in current_var_distribution:\n",
    "        for i, value in enumerate(sample):\n",
    "            if i < 128:  # Only consider up to 128 positions\n",
    "                total_vars[i] += value\n",
    "\n",
    "    # Calculate average mean for each position\n",
    "    average_means = np.zeros(128)\n",
    "    average_vars = np.zeros(128)\n",
    "\n",
    "    for i in range(128):\n",
    "        if counts[i] > 0:\n",
    "            average_means[i] = total_means[i] / counts[i]\n",
    "            average_vars[i] = total_vars[i] / counts[i]\n",
    "            #average_normbias[i] = total_normbias[i] / counts[i]\n",
    "        # If count is 0, the average_means[i] remains 0\n",
    "    \n",
    "    # Calculate average normbias\n",
    "    average_normbias = np.mean(current_normbias_distribution, axis=0)\n",
    "\n",
    "    if layer == 0:\n",
    "        precomputed_mean_0_0.append(average_means)\n",
    "        precomputed_var_0_0.append(average_vars)\n",
    "        precomputed_normbias_0_0.append(average_normbias)\n",
    "        current_mean_distribution = mean_distribution_0_1\n",
    "        current_var_distribution = var_distribution_0_1\n",
    "        current_normbias_distribution = normbias_distribution_0_1\n",
    "    elif layer == 1:\n",
    "        precomputed_mean_0_1.append(average_means)\n",
    "        precomputed_var_0_1.append(average_vars)\n",
    "        precomputed_normbias_0_1.append(average_normbias)\n",
    "        current_mean_distribution = mean_distribution_1_0\n",
    "        current_var_distribution = var_distribution_1_0\n",
    "        current_normbias_distribution = normbias_distribution_1_0\n",
    "    elif layer == 2:\n",
    "        precomputed_mean_1_0.append(average_means)\n",
    "        precomputed_var_1_0.append(average_vars)\n",
    "        precomputed_normbias_1_0.append(average_normbias)\n",
    "        current_mean_distribution = mean_distribution_1_1\n",
    "        current_var_distribution = var_distribution_1_1\n",
    "        current_normbias_distribution = normbias_distribution_1_1\n",
    "    elif layer == 3:\n",
    "        precomputed_mean_1_1.append(average_means)\n",
    "        precomputed_var_1_1.append(average_vars)\n",
    "        precomputed_normbias_1_1.append(average_normbias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vy_mean(vy_distribution):\n",
    "    # Stack all vy tensors\n",
    "    stacked_vy = torch.stack(vy_distribution)\n",
    "    \n",
    "    # Compute mean along the first dimension (across all inputs)\n",
    "    mean_vy = torch.mean(stacked_vy, dim=0)\n",
    "    \n",
    "    return mean_vy.cpu().numpy()\n",
    "\n",
    "# Compute mean for each vy distribution\n",
    "precomputed_vy_0_0 = compute_vy_mean(vy_distribution_0_0)\n",
    "precomputed_vy_0_1 = compute_vy_mean(vy_distribution_0_1)\n",
    "precomputed_vy_1_0 = compute_vy_mean(vy_distribution_1_0)\n",
    "precomputed_vy_1_1 = compute_vy_mean(vy_distribution_1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save precomputed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"./emotion-precompute\"):\n",
    "    os.makedirs(\"./emotion-precompute\")\n",
    "\n",
    "np.savetxt(\"./emotion-precompute/layer0_selfoutput_mean.txt\", precomputed_mean_0_0, delimiter='\\n')\n",
    "np.savetxt(\"./emotion-precompute/layer0_output_mean.txt\", precomputed_mean_0_1, delimiter='\\n')\n",
    "np.savetxt(\"./emotion-precompute/layer1_selfoutput_mean.txt\", precomputed_mean_1_0, delimiter='\\n')\n",
    "np.savetxt(\"./emotion-precompute/layer1_output_mean.txt\", precomputed_mean_1_1, delimiter='\\n')\n",
    "\n",
    "np.savetxt(\"./emotion-precompute/layer0_selfoutput_var.txt\", precomputed_var_0_0, delimiter='\\n')\n",
    "np.savetxt(\"./emotion-precompute/layer0_output_var.txt\", precomputed_var_0_1, delimiter='\\n')\n",
    "np.savetxt(\"./emotion-precompute/layer1_selfoutput_var.txt\", precomputed_var_1_0, delimiter='\\n')\n",
    "np.savetxt(\"./emotion-precompute/layer1_output_var.txt\", precomputed_var_1_1, delimiter='\\n')\n",
    "\n",
    "np.savetxt(\"./emotion-precompute/layer0_selfoutput_vy.txt\", precomputed_vy_0_0, delimiter=',')\n",
    "np.savetxt(\"./emotion-precompute/layer0_output_vy.txt\", precomputed_vy_0_1, delimiter=',')\n",
    "np.savetxt(\"./emotion-precompute/layer1_selfoutput_vy.txt\", precomputed_vy_1_0, delimiter=',')\n",
    "np.savetxt(\"./emotion-precompute/layer1_output_vy.txt\", precomputed_vy_1_1, delimiter=',')\n",
    "\n",
    "np.savetxt(\"./emotion-precompute/layer0_selfoutput_normbias.txt\", precomputed_normbias_0_0, delimiter='\\n')\n",
    "np.savetxt(\"./emotion-precompute/layer0_output_normbias.txt\", precomputed_normbias_0_1, delimiter='\\n')\n",
    "np.savetxt(\"./emotion-precompute/layer1_selfoutput_normbias.txt\", precomputed_normbias_1_0, delimiter='\\n')\n",
    "np.savetxt(\"./emotion-precompute/layer1_output_normbias.txt\", precomputed_normbias_1_1, delimiter='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision\n",
    "def precision(correct, approx):\n",
    "    if isinstance(approx, list):\n",
    "        approx = np.array(approx)\n",
    "    absolute = np.sum(np.abs(correct - approx)) / len(correct)\n",
    "    relative = absolute / (np.sum(np.abs(correct)) / len(correct))\n",
    "    return 1 - relative\n",
    "\n",
    "# Load real mean\n",
    "real_mean_0_0 = np.loadtxt(\"./weights-sst2/layer0_selfoutput_mean.txt\")\n",
    "real_mean_0_1 = np.loadtxt(\"./weights-sst2/layer0_output_mean.txt\")\n",
    "real_mean_1_0 = np.loadtxt(\"./weights-sst2/layer1_selfoutput_mean.txt\")\n",
    "real_mean_1_1 = np.loadtxt(\"./weights-sst2/layer1_output_mean.txt\")\n",
    "\n",
    "# Load real vy & normbias\n",
    "real_vy_0_0 = np.loadtxt(\"./weights-sst2/layer0_selfoutput_vy.txt\", delimiter=',')\n",
    "real_vy_0_1 = np.loadtxt(\"./weights-sst2/layer0_output_vy.txt\", delimiter=',')\n",
    "real_vy_1_0 = np.loadtxt(\"./weights-sst2/layer1_selfoutput_vy.txt\", delimiter=',')\n",
    "real_vy_1_1 = np.loadtxt(\"./weights-sst2/layer1_output_vy.txt\", delimiter=',')\n",
    "\n",
    "real_normbias_0_0 = np.loadtxt(\"./weights-sst2/layer0_selfoutput_normbias.txt\", delimiter=',')\n",
    "real_normbias_0_1 = np.loadtxt(\"./weights-sst2/layer0_output_normbias.txt\", delimiter=',')\n",
    "real_normbias_1_0 = np.loadtxt(\"./weights-sst2/layer1_selfoutput_normbias.txt\", delimiter=',')\n",
    "real_normbias_1_1 = np.loadtxt(\"./weights-sst2/layer1_output_normbias.txt\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision\n",
    "mean1 = precision(real_mean_0_0, precomputed_mean_0_0)\n",
    "mean2 = precision(real_mean_0_1, precomputed_mean_0_1)\n",
    "mean3 = precision(real_mean_1_0, precomputed_mean_1_0)\n",
    "mean4 = precision(real_mean_1_1, precomputed_mean_1_1)\n",
    "\n",
    "vy1 = precision(real_vy_0_0, precomputed_vy_0_0)\n",
    "vy2 = precision(real_vy_0_1, precomputed_vy_0_1)\n",
    "vy3 = precision(real_vy_1_0, precomputed_vy_1_0)\n",
    "vy4 = precision(real_vy_1_1, precomputed_vy_1_1)\n",
    "\n",
    "normbias1 = precision(real_normbias_0_0, precomputed_normbias_0_0)\n",
    "normbias2 = precision(real_normbias_0_1, precomputed_normbias_0_1)\n",
    "normbias3 = precision(real_normbias_1_0, precomputed_normbias_1_0)\n",
    "normbias4 = precision(real_normbias_1_1, precomputed_normbias_1_1)\n",
    "\n",
    "print(f\"Precision of mean_0_0: {mean1}\")\n",
    "print(f\"Precision of mean_0_1: {mean2}\")\n",
    "print(f\"Precision of mean_1_0: {mean3}\")\n",
    "print(f\"Precision of mean_1_1: {mean4}\")\n",
    "\n",
    "print(f\"Precision of vy_0_0: {vy1}\")\n",
    "print(f\"Precision of vy_0_1: {vy2}\")\n",
    "print(f\"Precision of vy_1_0: {vy3}\")\n",
    "print(f\"Precision of vy_1_1: {vy4}\")\n",
    "\n",
    "print(f\"Precision of normbias_0_0: {normbias1}\")\n",
    "print(f\"Precision of normbias_0_1: {normbias2}\")\n",
    "print(f\"Precision of normbias_1_0: {normbias3}\")\n",
    "print(f\"Precision of normbias_1_1: {normbias4}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
