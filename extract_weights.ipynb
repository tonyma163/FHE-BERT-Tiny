{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6858139-6463-481e-b526-8c562e7f671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Load fine-tuned model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe4a2102-7852-4e15-b3cc-c8c361b13acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your Python script that defines the BERT model\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "\n",
    "config = BertConfig(\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=2,  # Increase from 2 to 4\n",
    "    num_attention_heads=2,\n",
    "    intermediate_size=512,\n",
    "    max_position_embeddings=512,\n",
    "    num_labels=2,\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cde0a94c-5ebc-4aef-9bcd-7d119c75513c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ae85e60-5995-4bb5-8976-994ed10a28ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71a7e276-c3e2-41e6-a464-9cf0474d347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.eval()\n",
    "\n",
    "## Layer 0-3\n",
    "# layer0_attself_query_weight\n",
    "# layer0_attself_query_bias\n",
    "# layer0_attself_key_weight\n",
    "# layer0_attself_key_bias\n",
    "# layer0_attself_value_weight\n",
    "# layer0_attself_value_bias\n",
    "\n",
    "# layer0_selfoutput_weight\n",
    "# layer0_selfoutput_bias\n",
    "# layer0_selfoutput_mean\n",
    "# layer0_selfoutput_normbias\n",
    "# layer0_selfoutput_vy\n",
    "\n",
    "# layer0_intermediate_weight1\n",
    "# layer0_intermediate_weight2\n",
    "# layer0_intermediate_weight3\n",
    "# layer0_intermediate_weight4\n",
    "# layer0_intermediate_bias\n",
    "\n",
    "# layer0_output_weight1\n",
    "# layer0_output_weight2\n",
    "# layer0_output_weight3\n",
    "# layer0_output_weight4\n",
    "# layer0_output_bias\n",
    "# layer0_output_mean\n",
    "# layer0_output_normbias\n",
    "# layer0_output_vy\n",
    "\n",
    "## Pooler\n",
    "# pooler_dense_weight\n",
    "# pooler_dense_bias\n",
    "\n",
    "## Classifier\n",
    "# classifier_weight\n",
    "# classifier_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13fa03b4-53c0-4ac7-b224-f771d317bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"stanfordnlp/sst2\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee2f5f36-c886-4ad3-80bb-5273c1a403b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Create a dataloader\n",
    "train_dataloader = DataLoader(tokenized_train_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c14776cb-c525-4cbc-b054-9d3489857b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store LayerNorm Inputs\n",
    "layernorm_inputs = {\n",
    "    'layer0_self_output': [],\n",
    "    'layer0_output': [],\n",
    "    'layer1_self_output': [],\n",
    "    'layer1_output': []\n",
    "}\n",
    "\n",
    "# Hook function to capture the inputs from each LayerNorm layer\n",
    "def get_layernorm_input(layer):\n",
    "    def hook(module, input):\n",
    "        layernorm_inputs[layer].append(input[0].detach().cpu())\n",
    "    return hook\n",
    "layer0_self_output_hook = model.bert.encoder.layer[0].attention.output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer0_self_output')\n",
    ")\n",
    "layer0_output_hook = model.bert.encoder.layer[0].output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer0_output')\n",
    ")\n",
    "layer1_self_output_hook = model.bert.encoder.layer[1].attention.output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer1_self_output')\n",
    ")\n",
    "layer1_output_hook = model.bert.encoder.layer[1].output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer1_output')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51c7e3b7-12f9-4652-aa5b-e2e03b367a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2105/2105 [01:08<00:00, 30.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Process\n",
    "attention_mask_list = [] # to excludle padding tokens\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(\"cpu\")\n",
    "        attention_mask = batch['attention_mask'].to(\"cpu\")\n",
    "\n",
    "        attention_mask_list.append(attention_mask.detach())\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Clean the hooks\n",
    "layer0_self_output_hook.remove()\n",
    "layer0_output_hook.remove()\n",
    "layer1_self_output_hook.remove()\n",
    "layer1_output_hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8779b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# to excludle padding tokens\n",
    "all_attention_masks = torch.cat(attention_mask_list, dim=0)\n",
    "all_attention_masks = all_attention_masks.view(-1)\n",
    "\n",
    "# Compute the mean & inverse sqrt variance for each LayerNorm layer\n",
    "for layer, input_list in layernorm_inputs.items():\n",
    "    # concatenate all inputs for the current layer\n",
    "    all_inputs = torch.cat(input_list, dim=0)\n",
    "\n",
    "    # flatten the inputs to merge batch and sequence dimensions\n",
    "    total_samples, seq_length, hidden_size = all_inputs.shape\n",
    "    all_inputs = all_inputs.view(-1, hidden_size)\n",
    "\n",
    "    # exclude padding tokens\n",
    "    valid_indices = all_attention_masks.nonzero(as_tuple=False).squeeze()\n",
    "    valid_inputs = all_inputs[valid_indices]\n",
    "\n",
    "    # Compute mean and variance across all tokens and samples for each feature\n",
    "    mean = valid_inputs.mean(dim=0).double()\n",
    "    var = valid_inputs.var(dim=0, unbiased=False).double()\n",
    "\n",
    "    # Compute the inverse square root of variance + epsilon\n",
    "    epsilon = 1e-12\n",
    "    inv_sqrt_var = 1.0 / torch.sqrt(var + epsilon)\n",
    "    \n",
    "    #print(layer)\n",
    "    ln = None\n",
    "    # Compute vy & normbias\n",
    "    if (layer == \"layer0_self_output\"):\n",
    "        ln = model.bert.encoder.layer[0].attention.output.LayerNorm\n",
    "    elif (layer == \"layer0_output\"):\n",
    "        ln = model.bert.encoder.layer[0].output.LayerNorm\n",
    "    elif (layer == \"layer1_self_output\"):\n",
    "        ln = model.bert.encoder.layer[1].attention.output.LayerNorm\n",
    "    elif (layer == \"layer1_output\"):\n",
    "        ln = model.bert.encoder.layer[1].output.LayerNorm\n",
    "    \n",
    "    gamma = ln.weight.clone().detach().double()\n",
    "    beta = ln.bias.clone().detach().double()\n",
    "\n",
    "    path = \"./weightss\"\n",
    "    if not (os.path.exists(path)):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # Compute vy\n",
    "    vy = gamma * inv_sqrt_var\n",
    "    normbias = beta\n",
    "\n",
    "    # Modify layer name for filenames\n",
    "    layer_filename = layer.replace('self_output', 'selfoutput')\n",
    "\n",
    "    # Save vy and normbias to text files\n",
    "    np.savetxt(f\"{path}/{layer_filename}_vy.txt\", vy.numpy(), delimiter=',', fmt='%.18e')\n",
    "    np.savetxt(f\"{path}/{layer_filename}_normbias.txt\", normbias.numpy(), delimiter=',', fmt='%.18e')\n",
    "\n",
    "    # Save the means & inverse sqrt variance to text files (optional, for debugging)\n",
    "    np.savetxt(f\"{path}/{layer_filename}_mean.txt\", mean[:hidden_size].numpy(), delimiter=',', fmt='%.18e')\n",
    "    np.savetxt(f\"{path}/{layer_filename}_inv_sqrt_var.txt\", inv_sqrt_var[:hidden_size].numpy(), delimiter=',', fmt='%.18e')\n",
    "\n",
    "\n",
    "print(\"completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "632c325b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "gamma = model.bert.encoder.layer[0].attention.output.LayerNorm.weight.clone().detach().double()\n",
    "print(gamma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd8cee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "Number of non-zero values: 128\n",
      "0.0078125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check sst2 vy\n",
    "vy = np.loadtxt(f\"./weightss/layer0_selfoutput_vy.txt\", delimiter=',')\n",
    "\n",
    "print(vy.shape)\n",
    "\n",
    "# Count non-zero values\n",
    "non_zero_count = np.count_nonzero(vy)\n",
    "print(f\"Number of non-zero values: {non_zero_count}\")\n",
    "\n",
    "print(non_zero_count/(128*128))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
