{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27cd04-08f7-4ed3-98cf-67a6cc9633e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Valid the mentioned accuracy on huggingface repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d2060-a46a-4bfc-a52c-08aac2615d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip transformers datasets torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14992236-c3cc-4000-b480-99858607d1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/FHE-BERT-Tiny/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tony/FHE-BERT-Tiny/env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gokuls/BERT-tiny-emotion-intent\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gokuls/BERT-tiny-emotion-intent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db495e7-ee8b-48b0-a2f1-32ae6602ad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=128, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c04cbe-4bed-4334-adac-914303b0ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "valid_dataset = load_dataset(\"dair-ai/emotion\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7a9afdc-675f-4beb-8506-b76e2fd7231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "{'text': 'im feeling quite sad and sorry for myself but ill snap out of it soon', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(valid_dataset)\n",
    "print(valid_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68c61133-606a-4fc9-bdc3-0ccf4209270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding='max_length', max_length=128)\n",
    "tokenized_valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_valid_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Create a dataloader\n",
    "valid_dataloader = DataLoader(tokenized_valid_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06727e6-065d-40ce-9f10-495020c7b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "import torch\n",
    "\n",
    "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Store predictions and true labels\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in valid_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        logits = logits.detach().cpu()\n",
    "        label_ids = labels.to('cpu')\n",
    "        \n",
    "        # Store predictions and true labels\n",
    "        predictions.extend(torch.argmax(logits, dim=1).numpy())\n",
    "        true_labels.extend(label_ids.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c0ae424-4356-47d9-a7aa-8edd723a4011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9100\n",
      "Precision: 0.9117\n",
      "Recall:    0.9100\n",
      "F1-Score:  0.9102\n",
      "Confusion Matrix:\n",
      "[[516  12   3  10   7   2]\n",
      " [ 12 655  24   2   3   8]\n",
      " [  1  19 155   3   0   0]\n",
      " [ 13   4   1 253   4   0]\n",
      " [ 16   3   0   7 170  16]\n",
      " [  1   4   1   1   3  71]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.92      0.94      0.93       550\n",
      "         joy       0.94      0.93      0.94       704\n",
      "        love       0.84      0.87      0.86       178\n",
      "       anger       0.92      0.92      0.92       275\n",
      "        fear       0.91      0.80      0.85       212\n",
      "    surprise       0.73      0.88      0.80        81\n",
      "\n",
      "    accuracy                           0.91      2000\n",
      "   macro avg       0.88      0.89      0.88      2000\n",
      "weighted avg       0.91      0.91      0.91      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Calculate Precision, Recall, F1-Score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "\n",
    "# Generate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Detailed Classification Report\n",
    "report = classification_report(true_labels, predictions, target_names=valid_dataset.features['label'].names)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84949e26-5e69-435f-9acf-ebb7229f8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd66d4-c5b5-41ec-8e45-debb212ea06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Record the mean & inverse variance of each LayerNorm for further pre-compute LayerNorm computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3901b5b1-4412-41c1-862d-04b88078456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 16000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load train dataset\n",
    "train_dataset = load_dataset(\"dair-ai/emotion\", split=\"train\")\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b10e30f8-9f82-4d86-a15d-150e8933dda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Create a dataloader\n",
    "train_dataloader = DataLoader(tokenized_train_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90d73b41-cf71-4cc8-9020-5ce43550243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store LayerNorm Inputs\n",
    "layernorm_inputs = {\n",
    "    'layer0_self_output': [],\n",
    "    'layer0_output': [],\n",
    "    'layer1_self_output': [],\n",
    "    'layer1_output': []\n",
    "}\n",
    "\n",
    "# Hook function to capture the inputs from each LayerNorm layer\n",
    "def get_layernorm_input(layer):\n",
    "    def hook(module, input):\n",
    "        layernorm_inputs[layer].append(input[0].detach().cpu())\n",
    "    return hook\n",
    "layer0_self_output_hook = model.bert.encoder.layer[0].attention.output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer0_self_output')\n",
    ")\n",
    "layer0_output_hook = model.bert.encoder.layer[0].output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer0_output')\n",
    ")\n",
    "layer1_self_output_hook = model.bert.encoder.layer[1].attention.output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer1_self_output')\n",
    ")\n",
    "layer1_output_hook = model.bert.encoder.layer[1].output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer1_output')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "777dc084-2b40-4186-bde1-74a59f8d46ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 500/500 [00:11<00:00, 45.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Process\n",
    "attention_mask_list = [] # to excludle padding tokens\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        attention_mask_list.append(attention_mask.detach())\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Clean the hooks\n",
    "layer0_self_output_hook.remove()\n",
    "layer0_output_hook.remove()\n",
    "layer1_self_output_hook.remove()\n",
    "layer1_output_hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83cb6062-1f08-47ab-845a-d1bf29d6c117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# to excludle padding tokens\n",
    "all_attention_masks = torch.cat(attention_mask_list, dim=0)\n",
    "all_attention_masks = all_attention_masks.view(-1)\n",
    "\n",
    "# Compute the mean & inverse sqrt variance for each LayerNorm layer\n",
    "for layer, input_list in layernorm_inputs.items():\n",
    "    # concatenate all inputs for the current layer\n",
    "    all_inputs = torch.cat(input_list, dim=0)\n",
    "\n",
    "    # flatten the inputs to merge batch and sequence dimensions\n",
    "    total_samples, seq_length, hidden_size = all_inputs.shape\n",
    "    all_inputs = all_inputs.view(-1, hidden_size)\n",
    "\n",
    "    # exclude padding tokens\n",
    "    valid_indices = all_attention_masks.nonzero(as_tuple=False).squeeze()\n",
    "    valid_inputs = all_inputs[valid_indices]\n",
    "\n",
    "    # Compute mean and variance across all tokens and samples for each feature\n",
    "    mean = valid_inputs.mean(dim=0)\n",
    "    var = valid_inputs.var(dim=0, unbiased=False)\n",
    "\n",
    "    # Compute the inverse square root of variance + epsilon\n",
    "    epsilon = 1e-12\n",
    "    inv_sqrt_var = 1.0 / torch.sqrt(var + epsilon)\n",
    "\n",
    "    #\n",
    "    path = \"./precomputed_layernorm\"\n",
    "    if not (os.path.exists(path)):\n",
    "        os.makedirs(path)\n",
    "    # Save the means & inverse sqrt variance to text files\n",
    "    np.savetxt(f\"./{path}/{layer}_mean.txt\", mean.numpy())\n",
    "    np.savetxt(f\"./{path}/{layer}_inv_sqrt_var.txt\", inv_sqrt_var.numpy())\n",
    "\n",
    "print(\"completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851174e-b426-4a40-8464-7b154d4a3088",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the pre-computed LayerNorm performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "951e313f-258d-471e-b473-b835cbeebb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./precomputed_layernorm\"\n",
    "\n",
    "# Load precomputed means and variances\n",
    "mean_0_0 = np.loadtxt(f\"{path}/layer0_self_output_mean.txt\")\n",
    "inv_sqrt_var_0_0 = np.loadtxt(f\"{path}/layer0_self_output_inv_sqrt_var.txt\")\n",
    "\n",
    "mean_0_1 = np.loadtxt(f\"{path}/layer0_output_mean.txt\")\n",
    "inv_sqrt_var_0_1 = np.loadtxt(f\"{path}/layer0_output_inv_sqrt_var.txt\")\n",
    "\n",
    "mean_1_0 = np.loadtxt(f\"{path}/layer1_self_output_mean.txt\")\n",
    "inv_sqrt_var_1_0 = np.loadtxt(f\"{path}/layer1_self_output_inv_sqrt_var.txt\")\n",
    "\n",
    "mean_1_1 = np.loadtxt(f\"{path}/layer1_output_mean.txt\")\n",
    "inv_sqrt_var_1_1 = np.loadtxt(f\"{path}/layer1_output_inv_sqrt_var.txt\")\n",
    "\n",
    "#print(\"mean_0_0: \", mean_0_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "444bcd82-9f16-4df0-8d18-0767a6f721ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 2000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# the author used validation dataset to test the performance\n",
    "# I use test dataset\n",
    "test_dataset = load_dataset(\"dair-ai/emotion\", split=\"test\")\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7253f1c0-94ce-4888-9be5-d810b059a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy plain: 0.902\n",
      "Accuracy precom: 0.842\n",
      "Performance loss: 0.0665188470066519\n"
     ]
    }
   ],
   "source": [
    "correct_plain = 0\n",
    "correct_precomp = 0\n",
    "\n",
    "for ind in range(len(test_dataset)):\n",
    "    text = \"[CLS] \" + test_dataset['text'][ind] + \" [SEP]\"\n",
    "\n",
    "    tokenized = tokenizer(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    x = model.bert.embeddings(tokens_tensor, torch.tensor([[1] * len(tokenized_text)]))\n",
    "\n",
    "    key = model.bert.encoder.layer[0].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[0].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[0].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[0].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[0].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[0].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = x.double()\n",
    "\n",
    "    input_tensor = x.double()\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = torch.softmax(qk, -1)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "\n",
    "    mean = mean_0_0\n",
    "    var = inv_sqrt_var_0_0\n",
    "\n",
    "    w_output_dense = model.bert.encoder.layer[0].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[0].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        idx = i\n",
    "\n",
    "        if i > len(mean) - 1:\n",
    "            idx = len(mean) - 1\n",
    "\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[idx]) * var[idx]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[0].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].intermediate.dense.bias\n",
    "\n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[0].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "\n",
    "    mean = mean_0_1\n",
    "    var = inv_sqrt_var_0_1\n",
    "\n",
    "    fin7_whole = []\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        idx = i\n",
    "\n",
    "        if i > len(mean) - 1:\n",
    "            idx = len(mean) - 1\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[idx]) * var[idx]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    key = model.bert.encoder.layer[1].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[1].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[1].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[1].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[1].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[1].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = fin7_whole\n",
    "    input_tensor = fin7_whole\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = torch.softmax(qk, -1)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "\n",
    "    mean = mean_1_0\n",
    "    var = inv_sqrt_var_1_0\n",
    "\n",
    "    w_output_dense = model.bert.encoder.layer[1].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[1].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        idx = i\n",
    "\n",
    "        if i > len(mean) - 1:\n",
    "            idx = len(mean) - 1\n",
    "\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[idx]) * var[idx]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[1].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].intermediate.dense.bias\n",
    "\n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[1].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "\n",
    "    fin7_whole = []\n",
    "\n",
    "    mean = mean_1_1\n",
    "    var = inv_sqrt_var_1_1\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        idx = i\n",
    "\n",
    "        if i > len(mean) - 1:\n",
    "            idx = len(mean) - 1\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[idx]) * var[idx]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    densed_pooler = torch.tanh(torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias)\n",
    "\n",
    "    approx = densed_pooler[0][0].detach()\n",
    "    precise = model.bert.pooler(model.bert.encoder(x)[0]).detach()[0]\n",
    "\n",
    "    output = torch.matmul(approx, model.classifier.weight.transpose(0, 1).double()) + model.classifier.bias.double()\n",
    "    output_real = model(tokens_tensor, torch.tensor([[1] * len(tokenized_text)])).logits[0].detach()\n",
    "\n",
    "    # Multi-labels\n",
    "    predicted = torch.argmax(output).item()\n",
    "    predicted_real = torch.argmax(output_real).item()\n",
    "\n",
    "    if predicted_real == test_dataset['label'][ind]:\n",
    "      correct_plain += 1\n",
    "\n",
    "    if predicted == test_dataset['label'][ind]:\n",
    "      correct_precomp += 1\n",
    "\n",
    "    \"\"\"if output_real[0].item() > output_real[1].item() and test_dataset['label'][ind] == 0:\n",
    "        correct_plain = correct_plain + 1\n",
    "    if output_real[0].item() < output_real[1].item() and test_dataset['label'][ind] == 1:\n",
    "        correct_plain = correct_plain + 1\n",
    "\n",
    "    if output[0].item() > output[1].item() and test_dataset['label'][ind] == 0:\n",
    "        correct_precomp = correct_precomp + 1\n",
    "    if output[0].item() < output[1].item() and test_dataset['label'][ind] == 1:\n",
    "        correct_precomp = correct_precomp + 1\"\"\"\n",
    "\n",
    "accuracy_plain = float(correct_plain) / len(test_dataset['label'])\n",
    "accuracy_precomp = float(correct_precomp) / len(test_dataset['label'])\n",
    "\n",
    "print(\"Accuracy plain: {}\\nAccuracy precom: {}\".format(accuracy_plain, accuracy_precomp))\n",
    "\n",
    "print(\"Performance loss: {}\".format(1 - accuracy_precomp / accuracy_plain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a0953-e7fc-4ba9-857e-dff1bb0eeae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceptable result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110001c0-055a-4179-b064-8331ddede5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chebyshev Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "736ac3ad-2801-476a-a036-55ff0bbc1423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(correct, approx):\n",
    "    if type(approx) == list:\n",
    "        approx = np.array(approx)\n",
    "    absolute = sum(abs(correct - approx))/len(correct)\n",
    "    relative = absolute / (sum(abs(correct))/len(correct))\n",
    "    return 1 - relative\n",
    "\n",
    "def softmax_contribuisci(x, index = 1):\n",
    "    if index == 1:\n",
    "        input_exp_1.append(torch.max(x).item())\n",
    "        input_exp_1.append(torch.min(x).item())\n",
    "    else:\n",
    "        input_exp_2.append(torch.max(x).item())\n",
    "        input_exp_2.append(torch.min(x).item())\n",
    "\n",
    "    for head in x.squeeze():\n",
    "        for row in head:\n",
    "            if index == 1:\n",
    "                input_inv_1.append(torch.sum(torch.exp(row)).item())\n",
    "            else:\n",
    "                input_inv_2.append(torch.sum(torch.exp(row)).item())\n",
    "\n",
    "    return torch.softmax(x, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b84942-64e6-49b2-a633-57a0a3cf2d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_exp_1 = []\n",
    "input_inv_1 = []\n",
    "input_gelu_1 = np.array([])\n",
    "input_relu_1 = np.array([]) # new\n",
    "input_exp_2 = []\n",
    "input_inv_2 = []\n",
    "input_gelu_2 = np.array([])\n",
    "input_relu_2 = np.array([]) # new\n",
    "input_tanh = np.array([])\n",
    "\n",
    "fhe_correct = 0\n",
    "fhe_wrong = 0\n",
    "\n",
    "fhe_accuracy = 0\n",
    "std_accuracy = 0\n",
    "\n",
    "for ind in range(len(test_dataset)):\n",
    "    text = \"[CLS] \" + test_dataset['text'][ind] + \" [SEP]\"\n",
    "\n",
    "    tokenized = tokenizer(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "    x = model.bert.embeddings(tokens_tensor, torch.tensor([[1] * len(tokenized_text)]))\n",
    "\n",
    "    key = model.bert.encoder.layer[0].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[0].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[0].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[0].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[0].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[0].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = x.double()\n",
    "\n",
    "    input_tensor = x.double()\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "\n",
    "    #mean = np.array([-0.03383045433490704, -0.04689138747464171, -0.04320052751297194, -0.04194874763842685, -0.03849735236740709, -0.03583471496309556, -0.036673685450259945, -0.03533623114666153, -0.03301200050649906, -0.03385619903604035, -0.03394064677150061, -0.03581378040060232, -0.04000193681582013, -0.042994980738727644, -0.042689484809151766, -0.0422699887342667, -0.040702211423783496, -0.043257636922742766, -0.040924377288572664, -0.04212762593354266, -0.040090620729304687, -0.03727317047412721, -0.030603299343800818, -0.034141189654495016, -0.03468711091296442, -0.032307857857310274, -0.02926372943560165, -0.031292906450152466, -0.037837883896213766, -0.03745859562807607, -0.03794657692710982, -0.03860214509229593, -0.036185650111238955, -0.039154371235979875, -0.03589729976884486, -0.031731895884233016, -0.03465287223481833, -0.031348414682812194, -0.03688161652969029, -0.03338290816163936, -0.038240660222183975, -0.037525466450406116, -0.038229222217722264, -0.041201914113547705, -0.04212576296359885, -0.03980083151775188, -0.04072657806877826, -0.040145599490268025, -0.036685242667777444, -0.034109016054392725, -0.03544325775104831, -0.03623692053970561, -0.04948334692050963, -0.04596823422981405, -0.04892271117435003])\n",
    "    mean = mean_0_0\n",
    "    #var = np.array([0.7495962428549272, 0.6109555428467895, 0.6225590467577651, 0.62495153067201, 0.631395549935461, 0.634492711694546, 0.644892789064359, 0.6542099965205022, 0.6595559062153842, 0.6659906881037033, 0.6680168012366937, 0.6758412527257586, 0.6668118068796066, 0.6718192460326265, 0.67786737736941, 0.6808577853930836, 0.6736657333151266, 0.6676446046843724, 0.6659979061989304, 0.6743226078654423, 0.681388263935704, 0.6837117808950258, 0.6907147768934253, 0.684537831509984, 0.6896744328697597, 0.6916627127801457, 0.6954043965468235, 0.6954046755145293, 0.7001025287354249, 0.695094327647078, 0.6854203403085795, 0.7027792682295838, 0.6956849098218769, 0.6945153573872891, 0.6856697060013522, 0.6897353511373785, 0.700668908202082, 0.6965624918742969, 0.7082690699456209, 0.7043163331126293, 0.7070770512949652, 0.7042510307314358, 0.6978925459183357, 0.7205035876616076, 0.6902461198740245, 0.686971254827903, 0.7028843270104062, 0.7032880792671149, 0.7057843340136714, 0.7104860015626775, 0.7321738164781159, 0.71095817492914, 0.7401485084476891, 0.7312957890728539, 0.7375994654874705])\n",
    "    var = inv_sqrt_var_0_0\n",
    "\n",
    "    w_output_dense = model.bert.encoder.layer[0].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[0].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[0].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].intermediate.dense.bias\n",
    "\n",
    "    #input_gelu_1 = np.append(input_gelu_1, fin_4.reshape(-1).detach().numpy())\n",
    "    input_relu_1 = np.append(input_relu_1, fin_4.reshape(-1).detach().numpy())\n",
    "\n",
    "    #fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    relu_mult_1 = max(abs(min(input_relu_1)), abs(max(input_relu_1)))\n",
    "    fin_5 = 0.5 * (fin_4 / relu_mult_1) + 0.5 * (fin_4 / relu_mult_1) ** 2\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[0].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "\n",
    "    #mean = np.array([-0.09545516102868973, 0.034540955180462664, 0.03934738149667437, 0.040802318439555035, 0.04426037798445811, 0.04919343175846099, 0.0493616301294401, 0.047896279398118795, 0.04912640635535303, 0.048717249992826256, 0.0477219385203478, 0.05095357678578503, 0.05094908370417657, 0.0493275745992752, 0.048418324664654545, 0.0473653504669205, 0.04528009986283869, 0.04524247257539856, 0.046555073355952846, 0.0516135997743503, 0.049103903254210594, 0.048877585502238356, 0.048364988370661784, 0.049043507301742846, 0.049933470462367846, 0.05175179126331398, 0.05057227793143223, 0.055763206569478994, 0.055243365455213404, 0.04986745821758072, 0.047789218698650125, 0.047852162700887234, 0.04279460740337753, 0.04280733225675328, 0.04644169155736491, 0.04783492130826333, 0.04759649093761958, 0.045252139153821, 0.04367184005341422, 0.039034762655413016, 0.04374965234639466, 0.04355128435775863, 0.04499861862695065, 0.04318602336450084, 0.04549296197766528, 0.03907804279518851, 0.037683132925437485, 0.04109696491189214, 0.04410155617431274, 0.05015992918511731, 0.04335430986396108, 0.046492484403760526, 0.044277581701870204, 0.03723061917091777, 0.039156973130334664])\n",
    "    mean = mean_0_1\n",
    "    #var = np.array([0.4156698594967092, 0.7008452266859936, 0.7214270983257646, 0.7095727482866087, 0.7102521835201318, 0.710293676073547, 0.7091783271698753, 0.6973493176419543, 0.7011688527520855, 0.7007704875343309, 0.6950537183089973, 0.6948029158092094, 0.6919309911197036, 0.6933694537037308, 0.6970711644923971, 0.7004276850010867, 0.6964234913676165, 0.6987678419874651, 0.6951829293138483, 0.6973048809142951, 0.6989420799277399, 0.7005696487948311, 0.6993937733493811, 0.6902070532566239, 0.6958399824203775, 0.6900361005407983, 0.6925891359742274, 0.6831642926666377, 0.6865279710039072, 0.6904370385593245, 0.6963724536275457, 0.6948942601360332, 0.6784634186071326, 0.6759657478656234, 0.6828578884489792, 0.683566347862741, 0.6857777074044566, 0.672040915409448, 0.6784995422914343, 0.6732453264186854, 0.683881765911935, 0.6909411690410042, 0.6715428435769978, 0.6775867807314924, 0.6785015863916147, 0.676156117696202, 0.6786376609996214, 0.6763771062984715, 0.7119440584663215, 0.7070342067744777, 0.6895996022331654, 0.6683970656272868, 0.6695013664908844, 0.6566575067124804, 0.672887703816164])\n",
    "    var = inv_sqrt_var_0_1\n",
    "\n",
    "    fin7_whole = []\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    real = model.bert.encoder.layer[0](x)[0].transpose(1, 2).reshape(-1).detach()\n",
    "\n",
    "    #print(\"Precision: {}\".format(precision(real, fin7_whole[0].transpose(0, 1).reshape(-1).detach())))\n",
    "\n",
    "    key = model.bert.encoder.layer[1].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[1].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[1].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[1].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[1].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[1].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = fin7_whole\n",
    "    input_tensor = fin7_whole\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk, 2)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "\n",
    "    #mean = np.array([0.04805131047475803, 0.014145706172069285, 0.010630181813540026, 0.010521146572975027, 0.00956244983947186, 0.008211288558782809, 0.008817800275674387, 0.008911457532306733, 0.008643898058317862, 0.008801769546523253, 0.009472254700839258, 0.008094415948174241, 0.007702615754430344, 0.005460620353838359, 0.007021847370084451, 0.008373831982472147, 0.01022061224155272, 0.00927594903773269, 0.009277225000069925, 0.007049453120897054, 0.008682554190420182, 0.008749022040809715, 0.010118317324741522, 0.008998865743435887, 0.008763833543884292, 0.008285728555981435, 0.006967351876718886, 0.00588068616144895, 0.0030701809065725363, 0.003659716972971551, 0.002116778487431024, 0.003947434346765913, 0.006907859825079262, 0.008494112860837831, 0.007040283968419036, 0.007197681884381672, 0.008232685835987293, 0.009965029801574864, 0.00731962961637719, 0.00830555309310382, 0.005340440177451385, 0.007833324368720607, 0.01047456825511633, 0.009674864773662995, 0.010093537461664302, 0.01588798917017868, 0.018537933333636507, 0.018245848282989877, 0.012253993810893607, 0.011354133953173591, 0.013474744814287221, 0.013707011955501919, 0.007918842609048385, 0.017240907760895086, 0.03465881962238184])\n",
    "    mean = mean_1_0\n",
    "    #var = np.array([0.6741653046411179, 0.602392389437227, 0.5945841451997256, 0.5997135932136959, 0.6033806506910513, 0.6064839949503851, 0.6058735285405447, 0.6059001754921257, 0.6086086189801689, 0.6118981975241923, 0.6161533101614306, 0.6105411757987637, 0.6102443339235957, 0.6004337682468068, 0.6068584434133084, 0.6123178593290803, 0.6150302868629213, 0.6102744641580546, 0.6143169356654037, 0.6105845722771672, 0.61540315154488, 0.622109065598561, 0.6221720668578823, 0.6279330579960701, 0.6282907135959079, 0.6258439179151315, 0.6187239026398644, 0.618294817104495, 0.609488586748927, 0.6085185174201381, 0.6154275326252285, 0.6207534846328591, 0.6290521066315713, 0.6375810334496135, 0.6238236165346044, 0.6310571465398529, 0.6350551779511981, 0.6452639043477173, 0.6346915398812409, 0.646622546259538, 0.6435498445423712, 0.6401589932559348, 0.6458833892517316, 0.6354378204804867, 0.651796667347259, 0.6547600574517144, 0.6554038815336571, 0.655910889886979, 0.6412602949793637, 0.6489736968517984, 0.6633309254993116, 0.6771441398382873, 0.6423362709438692, 0.6302863730404997, 0.5940213893371686])\n",
    "    var = inv_sqrt_var_1_0\n",
    "\n",
    "    w_output_dense = model.bert.encoder.layer[1].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[1].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        #TODO QUA STO USANDO I VERI VALORI!!!!\n",
    "        #fin3_corr = (fin2.squeeze().detach() - torch.mean(fin2.squeeze())) / math.sqrt(torch.var(fin2.squeeze()))\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[1].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].intermediate.dense.bias\n",
    "\n",
    "    #input_gelu_2 = np.append(input_gelu_2, fin_4.reshape(-1).detach().numpy())\n",
    "    input_relu_2 = np.append(input_relu_2, fin_4.reshape(-1).detach().numpy())\n",
    "\n",
    "    #fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    relu_mult_2 = max(abs(min(input_relu_2)), abs(max(input_relu_2)))\n",
    "    fin_5 = 0.5 * (fin_4 / relu_mult_2) + 0.5 * (fin_4 / relu_mult_2) ** 2\n",
    "\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[1].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "\n",
    "    fin7_whole = []\n",
    "\n",
    "    mean = mean_1_1\n",
    "    var = inv_sqrt_var_1_1\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    real = model.bert.encoder.layer[1](model.bert.encoder.layer[0](x)[0])[0].transpose(1, 2).reshape(-1).detach()\n",
    "    correct = fin7_whole[0].transpose(0, 1).reshape(-1).detach()\n",
    "\n",
    "    input_tanh = np.append(input_tanh, (torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias).reshape(-1).detach())\n",
    "\n",
    "    #print(\"Precision: {}\".format(precision(real, correct)))\n",
    "    densed_pooler = torch.tanh(torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias)\n",
    "\n",
    "    approx = densed_pooler[0][0].detach()\n",
    "    precise = model.bert.pooler(model.bert.encoder(x)[0]).detach()[0]\n",
    "\n",
    "    #print(precision(precise, approx))\n",
    "\n",
    "    # Multi-labels\n",
    "    predicted = torch.argmax(output).item()\n",
    "    predicted_real = torch.argmax(output_real).item()\n",
    "\n",
    "    if predicted == test_dataset['label'][ind]:\n",
    "      fhe_accuracy += 1\n",
    "\n",
    "    if predicted_real == test_dataset['label'][ind]:\n",
    "      fhe_correct += 1\n",
    "    else:\n",
    "      fhe_wrong += 1\n",
    "\n",
    "    if predicted_real == test_dataset['label'][ind]:\n",
    "      std_accuracy += 1\n",
    "\n",
    "    \"\"\"\n",
    "    # 0/1\n",
    "    output = torch.matmul(approx, model.classifier.weight.transpose(0, 1).double()) + model.classifier.bias.double()\n",
    "    output_real = model(tokens_tensor, torch.tensor([[1] * len(tokenized_text)])).logits[0].detach()\n",
    "\n",
    "    if output[0].item() > output[1].item() and output_real[0].item() > output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    elif output[0].item() < output[1].item() and output_real[0].item() < output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    else:\n",
    "        fhe_wrong = fhe_wrong + 1\n",
    "\n",
    "    if output[0].item() > output[1].item() and dataset['label'][ind] == 0:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "    if output[0].item() < output[1].item() and dataset['label'][ind] == 1:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "\n",
    "    if output_real[0].item() > output_real[1].item() and dataset['label'][ind] == 0:\n",
    "        std_accuracy = std_accuracy + 1\n",
    "    if output_real[0].item() < output_real[1].item() and dataset['label'][ind] == 1:\n",
    "        std_accuracy = std_accuracy + 1\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5ec78-a7f2-4183-a964-bd1bd5af6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standard model accuracy:    {}\\nPrecomputed model accuracy: {}\".format(std_accuracy / len(dataset.index), fhe_accuracy / len(dataset.index)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
