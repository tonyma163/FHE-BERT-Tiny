{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6858139-6463-481e-b526-8c562e7f671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Load fine-tuned model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a2102-7852-4e15-b3cc-c8c361b13acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your Python script that defines the BERT model\n",
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "\n",
    "config = BertConfig(\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=4,  # Increase from 2 to 4\n",
    "    num_attention_heads=2,\n",
    "    intermediate_size=512,\n",
    "    max_position_embeddings=512,\n",
    "    num_labels=2,\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0a94c-5ebc-4aef-9bcd-7d119c75513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae85e60-5995-4bb5-8976-994ed10a28ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a7e276-c3e2-41e6-a464-9cf0474d347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.eval()\n",
    "\n",
    "## Layer 0-3\n",
    "# layer0_attself_query_weight\n",
    "# layer0_attself_query_bias\n",
    "# layer0_attself_key_weight\n",
    "# layer0_attself_key_bias\n",
    "# layer0_attself_value_weight\n",
    "# layer0_attself_value_bias\n",
    "\n",
    "# layer0_selfoutput_weight\n",
    "# layer0_selfoutput_bias\n",
    "# layer0_selfoutput_mean\n",
    "# layer0_selfoutput_normbias\n",
    "# layer0_selfoutput_vy\n",
    "\n",
    "# layer0_intermediate_weight1\n",
    "# layer0_intermediate_weight2\n",
    "# layer0_intermediate_weight3\n",
    "# layer0_intermediate_weight4\n",
    "# layer0_intermediate_bias\n",
    "\n",
    "# layer0_output_weight1\n",
    "# layer0_output_weight2\n",
    "# layer0_output_weight3\n",
    "# layer0_output_weight4\n",
    "# layer0_output_bias\n",
    "# layer0_output_mean\n",
    "# layer0_output_normbias\n",
    "# layer0_output_vy\n",
    "\n",
    "## Pooler\n",
    "# pooler_dense_weight\n",
    "# pooler_dense_bias\n",
    "\n",
    "## Classifier\n",
    "# classifier_weight\n",
    "# classifier_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa03b4-53c0-4ac7-b224-f771d317bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/sst2\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f5f36-c886-4ad3-80bb-5273c1a403b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['sentence'], truncation=True, padding='max_length', max_length=128)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531d4b3a-21cb-4e95-92a8-2226df35352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a dataloader\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df46453-575b-442d-8c0d-5825c296e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52462248-f9cd-40e2-82ed-a3c3beba1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def save_parameter(tensor, filename):\n",
    "    # Convert the tensor to a NumPy array\n",
    "    array = tensor.detach().cpu().numpy()\n",
    "    # Save the array to a text file\n",
    "    np.savetxt(filename, array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ece7be-8524-4862-97db-8c9324b78893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save the weights\n",
    "os.makedirs('weights', exist_ok=True)\n",
    "\n",
    "# For each encoder layer\n",
    "for layer_num in range(4):\n",
    "    prefix = f'bert.encoder.layer.{layer_num}.'\n",
    "    layer_prefix = f'layer{layer_num}_'\n",
    "    \n",
    "    # Self-Attention Weights and Biases\n",
    "    query_weight = state_dict[prefix + 'attention.self.query.weight']\n",
    "    query_bias = state_dict[prefix + 'attention.self.query.bias']\n",
    "    key_weight = state_dict[prefix + 'attention.self.key.weight']\n",
    "    key_bias = state_dict[prefix + 'attention.self.key.bias']\n",
    "    value_weight = state_dict[prefix + 'attention.self.value.weight']\n",
    "    value_bias = state_dict[prefix + 'attention.self.value.bias']\n",
    "    \n",
    "    save_parameter(query_weight, f'weights/{layer_prefix}attself_query_weight.txt')\n",
    "    save_parameter(query_bias, f'weights/{layer_prefix}attself_query_bias.txt')\n",
    "    save_parameter(key_weight, f'weights/{layer_prefix}attself_key_weight.txt')\n",
    "    save_parameter(key_bias, f'weights/{layer_prefix}attself_key_bias.txt')\n",
    "    save_parameter(value_weight, f'weights/{layer_prefix}attself_value_weight.txt')\n",
    "    save_parameter(value_bias, f'weights/{layer_prefix}attself_value_bias.txt')\n",
    "    \n",
    "    # Self-Output Weights and Biases\n",
    "    self_output_weight = state_dict[prefix + 'attention.output.dense.weight']\n",
    "    self_output_bias = state_dict[prefix + 'attention.output.dense.bias']\n",
    "    \n",
    "    save_parameter(self_output_weight, f'weights/{layer_prefix}selfoutput_weight.txt')\n",
    "    save_parameter(self_output_bias, f'weights/{layer_prefix}selfoutput_bias.txt')\n",
    "    \n",
    "    # Intermediate Weights and Biases\n",
    "    intermediate_weight = state_dict[prefix + 'intermediate.dense.weight']\n",
    "    intermediate_bias = state_dict[prefix + 'intermediate.dense.bias']\n",
    "    \n",
    "    # Since the intermediate weight is 128 x 512, we'll split it into four 128 x 128 blocks\n",
    "    intermediate_weight_blocks = torch.split(intermediate_weight, 128, dim=1)\n",
    "    for i, block in enumerate(intermediate_weight_blocks):\n",
    "        save_parameter(block, f'weights/{layer_prefix}intermediate_weight{i+1}.txt')\n",
    "    save_parameter(intermediate_bias, f'weights/{layer_prefix}intermediate_bias.txt')\n",
    "    \n",
    "    # Output Weights and Biases\n",
    "    output_weight = state_dict[prefix + 'output.dense.weight']\n",
    "    output_bias = state_dict[prefix + 'output.dense.bias']\n",
    "    \n",
    "    # Split the output weight into four 128 x 128 blocks\n",
    "    output_weight_blocks = torch.split(output_weight, 128, dim=0)\n",
    "    for i, block in enumerate(output_weight_blocks):\n",
    "        save_parameter(block, f'weights/{layer_prefix}output_weight{i+1}.txt')\n",
    "    save_parameter(output_bias, f'weights/{layer_prefix}output_bias.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd24c8-29bf-4c78-82a2-1d379ea28455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute mean & inv_sqrt_var & vy & normbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14776cb-c525-4cbc-b054-9d3489857b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store LayerNorm Inputs\n",
    "layernorm_inputs = {\n",
    "    'layer0_self_output': [],\n",
    "    'layer0_output': [],\n",
    "    'layer1_self_output': [],\n",
    "    'layer1_output': [],\n",
    "    'layer2_self_output': [],\n",
    "    'layer2_output': [],\n",
    "    'layer3_self_output': [],\n",
    "    'layer3_output': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7e3b7-12f9-4652-aa5b-e2e03b367a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function to capture the inputs from each LayerNorm layer\n",
    "def get_layernorm_input(layer):\n",
    "    def hook(module, input):\n",
    "        layernorm_inputs[layer].append(input[0].detach())\n",
    "    return hook\n",
    "layer0_self_output_hook = model.bert.encoder.layer[0].attention.output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer0_self_output')\n",
    ")\n",
    "layer0_output_hook = model.bert.encoder.layer[0].output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer0_output')\n",
    ")\n",
    "layer1_self_output_hook = model.bert.encoder.layer[1].attention.output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer1_self_output')\n",
    ")\n",
    "layer1_output_hook = model.bert.encoder.layer[1].output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer1_output')\n",
    ")\n",
    "layer2_self_output_hook = model.bert.encoder.layer[2].attention.output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer2_self_output')\n",
    ")\n",
    "layer2_output_hook = model.bert.encoder.layer[2].output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer2_output')\n",
    ")\n",
    "layer3_self_output_hook = model.bert.encoder.layer[3].attention.output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer3_self_output')\n",
    ")\n",
    "layer3_output_hook = model.bert.encoder.layer[3].output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer3_output')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34d465-0fa9-4516-a154-326d5d81ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model.eval()\n",
    "# Process\n",
    "attention_mask_list = [] # to excludle padding tokens\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        \n",
    "        attention_mask_list.append(attention_mask.detach())\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Clean the hooks\n",
    "layer0_self_output_hook.remove()\n",
    "layer0_output_hook.remove()\n",
    "layer1_self_output_hook.remove()\n",
    "layer1_output_hook.remove()\n",
    "layer2_self_output_hook.remove()\n",
    "layer2_output_hook.remove()\n",
    "layer3_self_output_hook.remove()\n",
    "layer3_output_hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd718d-2c33-4e46-969b-1a230f190336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to excludle padding tokens\n",
    "all_attention_masks = torch.cat(attention_mask_list, dim=0)\n",
    "all_attention_masks = all_attention_masks.view(-1)\n",
    "\n",
    "# Compute the mean & inverse sqrt variance for each LayerNorm layer\n",
    "for layer, input_list in layernorm_inputs.items():\n",
    "    # concatenate all inputs for the current layer\n",
    "    all_inputs = torch.cat(input_list, dim=0)\n",
    "\n",
    "    # flatten the inputs to merge batch and sequence dimensions\n",
    "    total_samples, seq_length, hidden_size = all_inputs.shape\n",
    "    all_inputs = all_inputs.view(-1, hidden_size)\n",
    "    \n",
    "    # exclude padding tokens\n",
    "    valid_indices = all_attention_masks.nonzero(as_tuple=False).squeeze()\n",
    "    valid_inputs = all_inputs[valid_indices]\n",
    "\n",
    "    # Compute mean and variance across all tokens and samples for each feature\n",
    "    mean = valid_inputs.mean(dim=0)\n",
    "    var = valid_inputs.var(dim=0, unbiased=False)\n",
    "\n",
    "    # Compute the inverse square root of variance + epsilon\n",
    "    epsilon = 1e-12\n",
    "    inv_sqrt_var = 1.0 / torch.sqrt(var + epsilon)\n",
    "\n",
    "    #\n",
    "    path = \"./weightss\"\n",
    "    if not (os.path.exists(path)):\n",
    "        os.makedirs(path)\n",
    "    # Save the means & inverse sqrt variance to text files\n",
    "    np.savetxt(f\"./{path}/{layer}_mean.txt\", mean.numpy())\n",
    "    np.savetxt(f\"./{path}/{layer}_inv_sqrt_var.txt\", inv_sqrt_var.numpy())\n",
    "\n",
    "print(\"completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
