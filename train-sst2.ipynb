{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonyma/code/FHE-BERT-Tiny/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/3t/qv8zrd4d5t12dqxjq4rxf56r0000gn/T/ipykernel_58607/4077033819.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trained = torch.load('./notebooks/SST-2-BERT-tiny.bin', map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, logging\n",
    "logging.set_verbosity_error() #Otherwise it will log annoying warnings\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "trained = torch.load('./notebooks/SST-2-BERT-tiny.bin', map_location=torch.device('cpu'))\n",
    "trained.pop('bert.embeddings.position_ids', None) # Remove unexpected keys\n",
    "model.load_state_dict(trained , strict=True)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = load_dataset(\"stanfordnlp/sst2\", split=\"train\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['sentence'], truncation=True, padding='max_length', max_length=128)\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Create a dataloader\n",
    "train_dataloader = DataLoader(tokenized_train_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store LayerNorm Inputs\n",
    "layernorm_inputs = {\n",
    "    'layer0_self_output': [],\n",
    "    'layer0_output': [],\n",
    "    'layer1_self_output': [],\n",
    "    'layer1_output': []\n",
    "}\n",
    "\n",
    "# Hook function to capture the inputs from each LayerNorm layer\n",
    "def get_layernorm_input(layer):\n",
    "    def hook(module, input):\n",
    "        layernorm_inputs[layer].append(input[0].detach().cpu())\n",
    "    return hook\n",
    "layer0_self_output_hook = model.bert.encoder.layer[0].attention.output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer0_self_output')\n",
    ")\n",
    "layer0_output_hook = model.bert.encoder.layer[0].output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer0_output')\n",
    ")\n",
    "layer1_self_output_hook = model.bert.encoder.layer[1].attention.output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer1_self_output')\n",
    ")\n",
    "layer1_output_hook = model.bert.encoder.layer[1].output.LayerNorm.register_forward_pre_hook(\n",
    "    get_layernorm_input('layer1_output')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2105/2105 [00:26<00:00, 80.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Process\n",
    "attention_mask_list = [] # to excludle padding tokens\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(\"cpu\")\n",
    "        attention_mask = batch['attention_mask'].to(\"cpu\")\n",
    "\n",
    "        attention_mask_list.append(attention_mask.detach())\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Clean the hooks\n",
    "layer0_self_output_hook.remove()\n",
    "layer0_output_hook.remove()\n",
    "layer1_self_output_hook.remove()\n",
    "layer1_output_hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# to excludle padding tokens\n",
    "all_attention_masks = torch.cat(attention_mask_list, dim=0)\n",
    "all_attention_masks = all_attention_masks.view(-1)\n",
    "\n",
    "# Compute the mean & inverse sqrt variance for each LayerNorm layer\n",
    "for layer, input_list in layernorm_inputs.items():\n",
    "    # concatenate all inputs for the current layer\n",
    "    all_inputs = torch.cat(input_list, dim=0)\n",
    "\n",
    "    # flatten the inputs to merge batch and sequence dimensions\n",
    "    total_samples, seq_length, hidden_size = all_inputs.shape\n",
    "    all_inputs = all_inputs.view(-1, hidden_size)\n",
    "\n",
    "    # exclude padding tokens\n",
    "    valid_indices = all_attention_masks.nonzero(as_tuple=False).squeeze()\n",
    "    valid_inputs = all_inputs[valid_indices]\n",
    "\n",
    "    # Compute mean and variance across all tokens and samples for each feature\n",
    "    mean = valid_inputs.mean(dim=0).double()\n",
    "    var = valid_inputs.var(dim=0, unbiased=False).double()\n",
    "\n",
    "    # Compute the inverse square root of variance + epsilon\n",
    "    epsilon = 1e-12\n",
    "    inv_sqrt_var = 1.0 / torch.sqrt(var + epsilon)\n",
    "    \n",
    "    #print(layer)\n",
    "    ln = None\n",
    "    # Compute vy & normbias\n",
    "    if (layer == \"layer0_self_output\"):\n",
    "        ln = model.bert.encoder.layer[0].attention.output.LayerNorm\n",
    "    elif (layer == \"layer0_output\"):\n",
    "        ln = model.bert.encoder.layer[0].output.LayerNorm\n",
    "    elif (layer == \"layer1_self_output\"):\n",
    "        ln = model.bert.encoder.layer[1].attention.output.LayerNorm\n",
    "    elif (layer == \"layer1_output\"):\n",
    "        ln = model.bert.encoder.layer[1].output.LayerNorm\n",
    "    \n",
    "    gamma = ln.weight.clone().detach().double()\n",
    "    beta = ln.bias.clone().detach().double()\n",
    "    \n",
    "    # Compute vy\n",
    "    vy = (gamma * inv_sqrt_var)\n",
    "    #normbias = beta - (gamma * mean * inv_sqrt_var)\n",
    "    normbias = beta\n",
    "    \n",
    "    # Expand vy to [128, 128]\n",
    "    max_length = 55\n",
    "    \"\"\"expanded_vy = vy.unsqueeze(0).repeat(max_length, 1)  # [55, 128]\n",
    "    padding_length = 128 - max_length\n",
    "    padding = torch.zeros(padding_length, 128, dtype=vy.dtype)\n",
    "    vy_expanded = torch.cat((expanded_vy, padding), dim=0)  # [128, 128]\"\"\"\n",
    "    # Expanded vy column-wise\n",
    "    expanded_vy = vy.unsqueeze(1).repeat(1, max_length)  # Shape: [128, 55]\n",
    "    padding_length = 128 - max_length\n",
    "    padding = torch.zeros(128, padding_length, dtype=vy.dtype)\n",
    "    vy_expanded = torch.cat((expanded_vy, padding), dim=1)  # Shape: [128, 128]\n",
    "\n",
    "\n",
    "    # Optionally, flatten to [128, 128] if needed by the HE circuit\n",
    "    # vy_expanded = vy_expanded.flatten()  # [16384]\n",
    "    #\n",
    "    path = \"./train-sst2\"\n",
    "    if not (os.path.exists(path)):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    # self_output -> selfoutput\n",
    "    layer = layer.replace('self_output', 'selfoutput')    \n",
    "        \n",
    "    # Save the means & inverse sqrt variance to text files\n",
    "    np.savetxt(f\"./{path}/{layer}_mean.txt\", mean.numpy())\n",
    "    np.savetxt(f\"./{path}/{layer}_inv_sqrt_var.txt\", inv_sqrt_var.numpy())\n",
    "    \n",
    "    # Save the vy & normbias\n",
    "    np.savetxt(f\"{path}/{layer}_vy.txt\", vy_expanded.numpy(), delimiter=',')\n",
    "    np.savetxt(f\"{path}/{layer}_normbias.txt\", normbias.numpy())\n",
    "\n",
    "print(\"completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
